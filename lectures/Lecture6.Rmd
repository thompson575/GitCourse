--- 
title:  "Lecture 6: "
editor_options: 
  chunk_output_type: console
---

<br>

This session is based on the paper,

Chen, Y., Xiang, J., Wang, Z., Xiao, Y., Zhang, D., Chen, X., Li, H., Liu, M. and Zhang, Q., 2015.  
**Associations of bone mineral density with lean mass, fat mass, and dietary patterns in postmenopausal Chinese women: a 2-year prospective study.**  
PloS one, 10(9), p.e0137097.  

The paper can be found at https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0137097 and the authors have made their data available by placing two Excel files in the dryad repository at https://datadryad.org/stash/dataset/doi:10.5061%2Fdryad.36k08.  

282 postmenopausal women from Harbin City, China were followed between 2009 and 2011 to study changes in bone mineral density (BMD). Dietary data were collected by food frequency questionnaires.

In this session we will cover,
 
* A simple function
* A basic shiny app      

<br>

<hr style = "border:2px solid #3559A6"> </hr>

<br>

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
```

<br>

<hr style = "border:2px solid #3559A6"> </hr>

<br>

# Cross-validation

Once again here is the toy datset from Lecture 3, that I also used in Lecture 5.

```{r}
tibble( id    = 1:8,
        sex   = c("m", "f", "m", "m", "f", "f", "f", "f"),
        age   = c( 45,  37,  25,  36,  52,  29,  53,  48),
        sbp   = c(135, 130, 120, 115, 155, 140, 145, 135) ) %>%
  mutate( sex = factor(sex, levels = c("m", "f"),
                            labels = c("male", "female"))) %>%
  print() -> sbpDF
```

Suppose that I were to fit a regression model to predict sbp from age and sex, how accurate would it be?  

Although lm() minimises the sum of the squared residuals (deviations), for illustration, I am going to measure model performance by the mean absolute deviation (MAD)
```{r}
# fit the model, report the results and calculate the MAD
library(broom)

sbpDF %>%
  lm( sbp ~ age + sex , data = . ) %>%
    {
    tidy(.) %>%
      print()
    
    glance(.) %>%
      print()

    augment(.) %>%
      summarise( mad = mean( abs(.resid )) ) %>%
      print()
    }
```

The MAD of 6.36 measures the average absolute difference between the measured SBP and the SBP predicted by the model.

The problem is that I fit the model with these 8 patients and then I assess the model performance with the same 8 people. It is to be expected that the fitted model would do less well if I applied it a new set of patients; I am over-fitting to this particular sample, so my performance measure is falsely optimistic.

To get a more realistic measure of model performance, I could split the data into 2 parts and use one part for fitting the model and the other for assessing the fit. I only have 8 patients so this approach is hardly practical, but I will do it for illustration. I'll use 6 patients for model fitting (training in the jargon) and 2 patients for assessing performance (validation in the jargon)

The `rsample` package has a function called `initial_split()` that does the job.

I want to use the split twice, once for the training data and once for the validation data, so I put everything in an anonymous function within my pipe.
```{r}
library(rsample)

# seed for reproducibility
set.seed(5629)

sbpDF %>%
  # split into training (6) and validation (2)
  initial_split( prop = 6/8 ) %>%
    {
    # fit model the the analysis (training) data
    analysis(.) %>%
      lm( sbp ~ age + sex , data = . ) -> model
    
    # assess performance with the validation data
    augment(model, newdata=assessment(.)) %>%
      summarise( mad = mean( abs(.resid )) ) %>%
      print()
    }
```

mean absolute deviation (MAD) is larger than before as expected, but the samples are so small that the result is very unreliable.

As this analysis is based on a single split, it could produce extreme results by chance and a better indication of performance would be obtained if we made lots of 6-2 splits and averaged the performance across them. This technique is known as **Monte Carlo Cross-Validation**.  

I start by giving my anonymous function a name.
```{r}
# -------------------------------------------------------
# function to fit on a sample of the data and assess 
# with the remainder
# arguments
#   split ... sample split into training and validation
# returns
#   the MAD of the validation data
#
fit_and_assess <- function( split) {
    # fit model the the analysis (training) data
    analysis(split) %>%
      lm( sbp ~ age + sex , data = . ) -> model
    
    # assess performance with the validation data
    augment(model, newdata=assessment(split)) %>%
      summarise( mad = mean( abs(.resid )) ) %>%
      pluck("mad") %>%
      return()
}

# -------------------------------------------------
# Run for 25 random 6/2 splits
set.seed(5629)

sbpDF %>%
  # create 25 splits
  mc_cv( prop = 6/8, times = 25 ) %>%
  # run fit_and_assess on each split
  mutate( mad = map_dbl( splits, fit_and_assess )) %>%
  print() %>%
  # average the 25 MADs
  summarise( meanMAD = mean(mad)) %>%
  print()
```

The average performance really is quite a lot worse than the original in-sample MAD had suggested.

A popular alternative to MC CV is to use k-fold cross-validation. Here you randomly split the sample to k parts (called folds). In our example, I divide the sample of 8 patients into 4 folds each of size 2. Then I work through the folds using the excluded pair for assessment and the other 6 for model fitting.

Almost everyone calls this k-fold cross-validation, but rsample has chosen to call it v-fold cross-validation. They are the same thing.

```{r}
# 4-fold cross-validation
set.seed(2908)

sbpDF %>%
  # create the 4 folds of size 2
  vfold_cv( v=4 ) %>% 
  print() %>%
  # find MAD for each fold and average
  mutate( mad = map_dbl( splits, fit_and_assess )) %>%
  summarise( meanMAD = mean(mad)) %>%
  print()
```

Again we see that using the same data for fitting and measuring performance exaggerated the performance of the model. Since this is averaged over 4 folds and the Monte Carlo version used 25 splits, I would expect the MC version to be a little more accurate.

<br>

<hr style = "border:2px solid #3559A6"> </hr>

<br>

# Ellipsis

Here is our simple coefficient of variation function from Lecture 4.

```{r}
# function to calculate the coefficient of variation (CV)
# arguments
#    x  a vector of numbers
# returns
#    cv  = sd/mean
#
cv <- function(x) {
  return( sd(x) / mean(x))
}
```

`mean()` has two extra arguments, `na.rm` to remove missing values before calculating the mean and `trim`, which removes the extreme values before the mean is calculated. `sd()` allows `na.rm` but not `trim`.

If I want to use these arguments in `cv()` then I could code it as
```{r}
# function to calculate the coefficient of variation (CV)
# arguments
#    x       a vector of numbers
#    na.rm   TRUE/FALSE remove NAs applies to mean and sd
#    trim    proportion of extremes to remove
# returns
#    cv  = sd/mean
#
cv <- function(x, na.rm = FALSE, trim = 0) {
  return( sd(x, na.rm) / mean(x, trim, na.rm))
}
```

This would work fine, but it would become very tedious if your function called many other functions each with multiple arguments. Your function could end up specifying dozens of arguments that you rarely use.

Here is the definition of `mean()` from its help file
```{r eval=FALSE}
mean(x, trim = 0, na.rm = FALSE, ...)
```

As well as the trim and na.rm arguments, mean allows the ellipsis, three dots. This is a notation that simplifies function writing. It says that when extra arguments are specified, pass them on. 

Most modern functions allow an ellipsis, but this is not the case with older base R functions. `mean()` does allow an ellipsis, but `sd()` does not.

```{r}
# function to calculate the coefficient of variation (CV)
# arguments
#    x      a vector of numbers
#    na.rm  TRUE/FALSE remove NAs applies to mean and sd
#    ...    further arguments to be passed on
# returns
#    cv  = sd/mean
#
cv <- function(x, na.rm = FALSE, ...) {
  return( sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm, ...))
}
```

This works as you would expect
```{r}
data <- c(3, 5, 6, 19, NA, 2, 2, 4, 1, 6, 7)

cv(data, na.rm=TRUE)
```

I can use the trim argument even though it is not mentioned explicitly in my function definition. It is passed as part of the ellipsis.

```{r}
cv(data, na.rm=TRUE, trim = 0.1)
```

In this case, you could specify a meaningless argument and it would be passed on. However, the way that R works is that when an ellipsis is allowed, unnecessary arguments are ignored, so the cabbage argument gets passed to `mean()` but it is ignored.

```{r}
cv(data, na.rm=TRUE, cabbage = FALSE)
```


# Simulation

Simulation is a useful method for examining the properties of an analysis. A typical use of simulation is when we plan an analysis and want to know how powerful it will be.  

Imagine that we plan a blood pressure study that will randomly recruit equal numbers of men and women aged between 50 and 65. The objective is to test whether there is a difference in SBP between the sexes after adjusting for age. How many people should we recruit?

To answer the question we need more information. Enough information to simulate a sample data set.

A quick internet search will tell you that average SBP is about 120 at age 50 and rises to about 145 at age 65, a rise of about 1.7 per year. Let's assume that

\[
SBP = 120 + 1.7*(age - 50) + \alpha [is \ \ male] + \epsilon
\]

where $\alpha$ measure the amount by which average male SBP is higher than average female SBP and $\epsilon$ captures the variation in SBP between individuals of the same age and sex.

Here is a simulated data set of with 5 men and 5 women, when $\alpha$=2mmHg and $\epsilon$ has a standard deviation of 12.
```{r}
# set seed for reproducibility
set.seed(4598) 

tibble( # ages uniformly spread between 50 and 65
        age = runif(10, 50, 65),
        # 5 men(1) and 5 women (0)
        sex = rep( 0:1, each=5),
        # random sbp according to the assumed model
        sbp = 120 + 1.7 * (age - 50) + 2 * sex + rnorm(10, 0, 12)) %>%
  # round the measurements into whole numbers
  mutate( age = round(age, 0),
          sbp = round(sbp, 0)) %>%
  print() -> trialDF
```

we can analyse these data as if they were a from a real study
```{r}
trialDF %>%
  lm( sbp ~ age + sex, data = .) %>%
  broom::tidy()
```

By chance these data show a lower SBP in men, even though I set $\alpha$=2 and the p-value for sex is not significant (p=0.261). 

Obviously the study is not large enough. None the less, it would be sensible to ask, if I conducted many such surveys, what proportion of them would give a significant result? In other words what is the power?

Let's put the simulation and analysis into a function.

```{r}
# function to simulate and analyse SBP data
# arguments
#    n    study size (mean + women)
# returns
#    tibble of regression coefficients
single_study <- function( n ) {
  # simulate the data
  tibble( age = runif(n, 50, 65),
        sex = rep( 0:1, each=n/2),
        sbp = 120 + 1.7 * (age - 50) + 2 * sex + rnorm(n, 0, 12)) %>%
  mutate( age = round(age, 0),
          sbp = round(sbp, 0)) %>%
  # analyse and return a table of regression coefficients
  lm( sbp ~ age + sex, data = .) %>%
  broom::tidy() %>%
  return()
}
```

Eventually, I will run a large number of simulations, but while I am developing the code I will restrict myself to 5 simulations, which I identify by `simNumber`

```{r}
# set seed for reproducibility
set.seed(4598) 

# run single_study 5 times 
tibble( n = rep(10, 5)) %>%
  mutate( coefs = map( n, single_study) ) %>% 
  print()
```

Next I need to `unnest()` the simulations to show all of the coefficients

```{r}
# set seed for reproducibility
set.seed(4598) 

# run single_study 5 times 
tibble( n = rep(10, 5)) %>%
  mutate( coefs = map( n, single_study) ) %>% 
  unnest(coefs) %>%
  print()
```

Finally, I find the proportion of times that a simulated study gives a significant sex term.
```{r}
# set seed for reproducibility
set.seed(4598) 

# run single_study 5 times 
tibble( n = rep(10, 5)) %>%
  mutate( coefs = map( n, single_study) ) %>% 
  unnest(coefs) %>%
  # just keep the sex rows
  filter( term == "sex" ) %>%
  # proportion of p-values below 0.05
  summarise( power = sum( p.value < 0.05 ) / 5 ) %>%
  print()
```

The power is 0%, so it looks like performing such a small study would be a waste of time. However this power estimate is based on just 5 simulations, so it will not be very reliable. To estimate the power more accurately requires a larger number of simulations

```{r}
# set seed for reproducibility
set.seed(4598) 

# run single_study 1000 times 
tibble( n = rep(10, 1000)) %>%
  mutate( coefs = map( n, single_study) ) %>% 
  unnest(coefs) %>%
  filter( term == "sex" ) %>%
  summarise( power = sum( p.value < 0.05 ) / 1000 ) %>%
  print()
```

4% power is not great; usually studies are only considered worthwhile if they have a power over 80%.

Perhaps a larger sample size would help. Let's try 100 participants, 50 men and 50 women.

```{r}
# set seed for reproducibility
set.seed(4598) 

# run single_study 1000 times 
tibble( n = rep(100, 1000)) %>%
  mutate( coefs = map( n, single_study) ) %>% 
  unnest(coefs) %>%
  filter( term == "sex" ) %>%
  summarise( power = sum( p.value < 0.05 ) / 1000 ) %>%
  print()
```

Rather disappointing. Even with 100 subjects I only have 13% power to detect a difference of 2mmHg.

Perhaps 2mmHg is a rather conservative guess, what if men had an average SBP 4 higher than women

I'll redefine `single_study()` to allow for alpha to be changed.
```{r}
# function to simulate and analyse SBP data
# arguments
#    n    study size (mean + women)
#    alpha mean difference in SBP between men and women
# returns
#    tibble of regression coefficients
single_study <- function( n, alpha ) {
  tibble( age = runif(n, 50, 65),
        sex = rep( 0:1, each=n/2),
        sbp = 120 + 1.7 * (age - 50) + alpha * sex + rnorm(n, 0, 12)) %>%
  mutate( age = round(age, 0),
          sbp = round(sbp, 0)) %>%
  lm( sbp ~ age + sex, data = .) %>%
  broom::tidy() %>%
  return()
}

set.seed(4598) 

tibble( n     = rep(100, 1000), 
        alpha = rep(  4, 1000)) %>%
  mutate( coefs = map2( n, alpha, single_study) ) %>%
  unnest(coefs) %>%
  filter( term == "sex" ) %>%
  summarise( power = sum( p.value < 0.05 ) / 1000 ) %>%
  print()
```

37% power is a bit more promising. Let's look at different values of alpha to see what difference we would have a good chance of detecting.

```{r}
power_sim <- function( nSim, n, alpha) {
  tibble( n     = rep(    n, 1000), 
          alpha = rep(alpha, 1000)) %>%
  mutate( coefs = map2( n, alpha, single_study) ) %>%
  unnest(coefs) %>%
  filter( term == "sex" ) %>%
  summarise( power = sum( p.value < 0.05 ) / nSim ) %>%
  pluck( "power" ) %>%
  return()
}

# set seed for reproducibility
set.seed(4598) 

# run single_study 1000 times 
tibble( alpha = 1:10) %>%
  mutate( power = map_dbl(alpha, ~ power_sim(1000, 100, alpha=.x))) %>%
  print() %>%
  ggplot( aes(x=alpha, y=power)) +
  geom_line() +
  geom_hline( yintercept = 0.8, linetype = "dashed")
```

So a study of 100 people would have about 80% power to detect a difference of 7mmHg.

<br>

<hr style = "border:2px solid #3559A6"> </hr>

<br>

# Functions than contain pipes

Consider these two pieces of code
```{r eval=FALSE}
# city is an address column within thisDF from which we extract all 
# of the rows where x is equal to thisCity
#
fun1 <- function(thisDF, thisCity) {
  thisDF %>%
    filter( city == thisCity ) %>%
    return()
}

# use fun1 to find all addresses where city is London
homeTown <- "London"
fun1( addressDF, homeTown)
```

When R sees the call to fun1, it needs to find the data for each argument. Let's assume that you have defines the tibble addressDF; the other job is to find the value of homeTown. homeTown is defined in the calling environment, so R will know to pass "London" into the function.

```{r eval=FALSE}
# thisCol represents a column within thisDF from which we extract 
# all of the rows equal to "London"
#
fun2 <- function(thisDF, thisCol) {
  thisDF %>%
    filter( thisCol == "London" ) %>%
    return()
}

# use fun2 to find all addresses where city is London
fun2( addressDF, city)
```

Once again R will locate addressDF and then it will look for `city` in the calling environment. Of course, R will not find it. `city` is not a free standing object, it is buried inside `thisDF`. R will not find `city` and the code will fail.

Rather than substitute a value into `filter( thisCol == "London")`, we actually need R to substitute the expression `city` so that the code reads `filter( city == "London")`, which makes sense within the context of that pipe.

Let's consider an example in which we want to pass reference to a column inside a data frame. 

Suppose that you are analysing a file containing measurements on different cytokines and over and over again you want the same type of plot

```{r}
# file of cytokine data used in Lecture 3
cytokine_rds <- "C:/Projects/RCourse/localISCB/data/rData/nijaguna_cytokine.rds"

# read the data
cytokineDF <- read_rds( cytokine_rds)

# find mean and standard deviation of IL3
cytokineDF %>%
  filter( !is.na(IL3)) %>%
  summarise( m = mean(IL3),
             s = sd(IL3)) %>%
  mutate( across(m:s, ~ round(.x, 2) ) ) %>%
  print() -> summaryDF

# create a character string summarising the results for IL3          
summaryText <- paste("mean =", summaryDF$m, "sd =", summaryDF$s)

# plot histogram of IL3 using the summary text as a title
cytokineDF %>%
  ggplot( aes(x = IL3)) +
  geom_histogram( bins = 50, fill="steelblue") +
  labs( title = summaryText)
```

It would make sense to place this code in a function, because there are 47 cytokines and we could use the function to plot a histogram of any cytokine that interests us. 

Here is some code that we know will not work. We pass `thisCytokine` as an argument, but it refers to a column of `thisDF` and not to a free-standing object.
```{r eval=FALSE}
myPlot <- function(thisDF, thisCytokine) {
  
thisDF %>%
  filter( !is.na(thisCytokine)) %>%
  summarise( m = mean(thisCytokine),
             s = sd(thisCytokine)) %>%
  mutate( across(m:s, ~ round(.x, 2) ) )  -> summaryDF
          
summaryText <- paste("mean =", summaryDF$m, "sd =", summaryDF$s)

thisDF %>%
  ggplot( aes(x = thisCytokine)) +
  geom_histogram( bins = 50, fill="steelblue") +
  labs( title = summaryText)
}

```

To use the function, you might try
```{r eval = FALSE}
myPlot( cytokineDF, IL3)
```

R uses **lazy evaluation**, which means that it just makes a note of the arguments but does not search for the data until it is needed.

So R tries to run your function and the first line refers to `thisDF`, which you set to `cytokineDF`. R looks in the calling environment for `cytokineDF`, finds it and all is well. The next line contains `!is.na(thisCytokine)` so R looks in the calling environment for `IL3`, but R will not find it because`IL3` is hidden away inside `cytokineDF`.

There is a simple notation that gets R to substitute the argument as an expression into the functions code without trying to evaluate it, which is double curly brackets, `{{ }}`. So the correct form of our function is

```{r}
myPlot <- function(thisDF, thisCytokine) {
  
thisDF %>%
  filter( !is.na( {{thisCytokine}} )) %>%
  summarise( m = mean( {{thisCytokine}} ),
             s = sd({{ thisCytokine}} )) %>%
  mutate( across(m:s, ~ round(.x, 2) ) )  -> summaryDF
          
summaryText <- paste("mean =", summaryDF$m, "sd =", summaryDF$s)

thisDF %>%
  ggplot( aes(x = {{thisCytokine}} )) +
  geom_histogram( bins = 50, fill="steelblue") +
  labs( title = summaryText)
}
```

Now we can run the functions and all is well.

```{r}
myPlot( cytokineDF, IL3)
```

Here I apply the same function to a different cytokine

```{r}
myPlot( cytokineDF, HGF)
```

This process of not looking for a variable in the calling environment is known as **non-standard evaluation** or NSE for short. It features a lot in the tidyverse.

<br>

<hr style = "border:2px solid #3559A6"> </hr>

<br>

# Patchwork

A brief rest from functions to look at the **patchwork** package. Patchwork enables you to combine separate ggplot visualisation into a single plot.

The code below creates three plots of cytokine data and saves them as `scatIL3HGF`, `histIL3` and `histHGF`. 
```{r}
# scatter plot x=IL3, y=HGF
cytokineDF %>%
  ggplot( aes(x = IL3, y=HGF)) +
  geom_point( colour="steelblue") -> scatIL3HGF

# histogram of IL3
cytokineDF %>%
  ggplot( aes(x = IL3)) +
  geom_histogram( bins = 50, fill="steelblue") -> histIL3

# histogram of HGF
cytokineDF %>%
  ggplot( aes(x = HGF)) +
  geom_histogram( bins = 50, fill="steelblue") -> histHGF
```

Now that they are saved we can plot them at any time.
```{r}
# display the saved scatter plot
print( scatIL3HGF)
```

However, sometimes it is nice to show the three plots together, which is where `patchwork` comes in.

The basic syntax is simple. We specify the plots in the order in which they are to be displayed using `+` to continue along the same row and `/` to stack the plots and `|` to place them side by side.

```{r}
library(patchwork)

# histograms side by side on the top row with the scatter plot below
( histIL3 + histHGF ) / scatIL3HGF 
```

or perhaps you prefer
```{r}
# stacked histograms alongside the scatter plot
( histIL3 / histHGF ) | scatIL3HGF
```

`patchwork` enables a host of refinements that you can read about at https://patchwork.data-imaginist.com/index.html

Here is an example of a few of the possibilities. Note the use of brackets around the basic patchwork. These are necessary so that the `plot_annotation()` applies to the whole page. & applies the theme to all of the individual plots.

```{r}
(( histIL3 / histHGF ) | scatIL3HGF ) + 
  plot_annotation(
     title      = 'Cytokines IL3 and HGF',
     subtitle   = 'there are 49 cytokines in total',
     caption    = 'Disclaimer: This is only an example',
     tag_prefix = 'Fig ',
     tag_levels = 'i' ) &
  theme_classic()
```

# Shiny

tabsets and adding text/html

reactivity

a cytokine dashboard