--- 
title: "Introduction: Intermediate level"
---

## The story so far

Lecture 1 to 3 covered the introductory material on data analysis with the tidyverse and stressed the use of the use of the pipe, %>%, to produce readable code.

Although the material covered so far is basic, it covers all that you need for most small scale data analyses. In fact, it provides the basis for everything that can be done without writing your own R functions. This is not a major limitations because the packages available in repositories such as CRAN include hundreds of thousands of functions that will perform most data analysis tasks.  

## Algorithmic thinking

A data analysis task usually involves several step; for instance, read the data, select the blood pressure measurements, remove the missing values, split the patients into men and women, calculate the means and print the result. There is a linear flow in the process so we code it as

```{r}
#| eval = false
read the data THEN
  select the BP measures THEN
  remove missing values THEN
  divide into mean and women THEN
  calculate the means THEN
  print the result
```

This process is a simple algorithm, a rule for going from a data file to some summary statistics that interest you. The process is transparent and easy to follow. The pipe enable us to maintain that transparency when we code the algorithm in R.

The R code might look like this
```{r}
#| eval = false
read_rds("myFile.rds") %>%
  select( gender, sbp, dbp) %>%
  filter( !is.na(sbp) & !is.na(dbp)) %>%
  group_by( gender) %>%
  summarise( meanSBP = mean(sbp),
             meanDBP = mean(dbp)) %>%
  print()
```

The code follows the algorithm and you can see EXACTLY what was done. We can see that if someone has a missing dbp they will be dropped from the analysis and even if the sbp is recorded, it will not contribute to the mean. You should also be able to see missing genders do not get dropped so if there are any the means would be calculated over males, females and missing.  

The ability to see exactly what you algorithm is doing is an essential part of good data analysis.

## User-written functions

The tidyverse provides a host of functions that work well in a pipe and R packages provide even more functions that we can fit into a pipe with a little ingenuity, but the next stage in the development of your R code is to write your own functions. 

There are three distinct reasons for wanting to write your own functions,  

* to expand what you can do within a pipe   
* to bring high-level structure to larger projects    
* to implement methods not available in base R or an existing package  

At an intermediate level, I like to stress the first of these aims; I consider the second and third aims to be more relevant to an advanced course. The skills in functions writing that you develop in this course will improve your pipes and they will prepare you for the advanced stage.

As we saw throughout the introductory lectures, the pipe provides a way of writing fast, concise and very legible code. At its best, code written with the pipe and tidyverse functions will be clearer, less likely to contain errors and it will be easier to maintain. Yet, there are tasks that are difficult to program given the methods that we have seen so far. For instance, we have not yet seen an efficient way of repeating similar steps within a pipe.  

Consider a trivial example,
```{r echo=FALSE}
# find the means of multiple clinical measures
myDF %>%
  summarise( meanAge  = mean(age),
             meanSbp  = mean(sbp),
             meanDbp  = mean(dbp),
             meanChol = mean(chol))
```

The code is clear, but repetitive. The more that you need to type, the greater the chance of an inadvertent typo. Imagine that there were 50 clinical measures to be averaged, this type of repetitive coding would work, but it would become difficult to read and it would be easy to mistype something and create an error that you would struggle to spot. 

Hadley Wickham advocates the **rule of three**; if you find yourself typing the same or very similar code three or more times, then look for a more efficient way of coding.  
 
We will see that efficient coding require a style known as functional programming (FP) and as the name suggests, to take full advantage of FP requires you to be able to write your own functions.  

## Writing a good function  

It is very easy to write a function in R, partly because R is a very flexible language and it will let you do almost anything, even when you choose to write code that will create problems.

Here is a very short function that calculates the coefficient of variation of a set of numbers. The coefficient of variation is just the standard deviation divided by the mean.
```{r}
# simple function
cv <- function(x) {
  return( sd(x) / mean(x) )
}

# using the function
tibble( id  = 1:6, 
        sbp = c(120, 145, 130, 130, 155, 115)) %>%
  summarise( n     = n(),
             cvSbp = cv(sbp))
```

For simple sets of data this works fine, but it is not difficult to see that this functions could easily breakdown.  

* what if x were to be a vector of strings?  
* what if x contains missing values?  
* what if x has length 1, so that the sd is undefined?  

We could add code to our function that would handle these problems, but R does not **force** us to do it.

One big advantage of working with functions is that we can test to see exactly how they will react

```{r}
cv( c(3, 6, 2, 1, 8))

cv( c(3, NA, 2, 1, 8))

cv( c("jack", "jill", "fred"))

cv(56)
```

